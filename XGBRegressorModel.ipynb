{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11f0258-6f77-4f75-bc32-b09b92902722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from xgboost import XGBRegressor\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3bb3028-297f-4f39-a08d-f7c0009ce79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy not available - using CPU arrays\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #Runs better without CuPy as this dataset is very small\n",
    "    import cupy as cp\n",
    "    gpu_available = True\n",
    "    print(\"CuPy available - GPU acceleration enabled\")\n",
    "except ImportError:\n",
    "    gpu_available = False\n",
    "    print(\"CuPy not available - using CPU arrays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9fad229-bb4c-48c3-a621-bea1516bf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_xgboost(data_frame, feature_columns, target_column, segment_column=None, train_ratio=0.8):\n",
    "    # Split data\n",
    "    data_split = int(len(data_frame) * train_ratio)\n",
    "    training_data, test_data = data_frame[:data_split], data_frame[data_split:]\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    X_train = training_data[feature_columns].values.astype(np.float32)\n",
    "    y_train = training_data[target_column].values.astype(np.float32)\n",
    "    segments_train = training_data[segment_column].values if segment_column else None\n",
    "    \n",
    "    X_test = test_data[feature_columns].values.astype(np.float32)\n",
    "    y_test = test_data[target_column].values.astype(np.float32)\n",
    "    segments_test = test_data[segment_column].values if segment_column else None\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"XGBOOST DATA PREPARATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Features: {feature_columns}\")\n",
    "    print(f\"Target: {target_column}\")\n",
    "    print(f\"Train shape: {X_train.shape}\")\n",
    "    print(f\"Test shape: {X_test.shape}\")\n",
    "    \n",
    "    if segment_column:\n",
    "        print(f\"Number of unique segments (train): {len(np.unique(segments_train))}\")\n",
    "        print(f\"Number of unique segments (test): {len(np.unique(segments_test))}\")\n",
    "    \n",
    "    # Convert to GPU arrays if available\n",
    "    if gpu_available:\n",
    "        X_train_gpu = cp.array(X_train)\n",
    "        y_train_gpu = cp.array(y_train)\n",
    "        X_test_gpu = cp.array(X_test)\n",
    "        print(\"Data converted to GPU arrays\")\n",
    "        return X_train_gpu, y_train_gpu, X_test_gpu, y_test, segments_train, segments_test, test_data\n",
    "    else:\n",
    "        return X_train, y_train, X_test, y_test, segments_train, segments_test, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3509bd2c-c2c4-46ad-9ae0-a96dc73bbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUGridSearchCV:\n",
    "    #Change verbose to 0 for no output for hyperparameter search and 1 for output\n",
    "    def __init__(self, base_params, param_grid, cv, scoring='neg_mean_absolute_error', verbose=0):\n",
    "        self.base_params = base_params\n",
    "        self.param_grid = param_grid\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.verbose = verbose\n",
    "        self.best_params_ = None\n",
    "        self.best_score_ = float('-inf')\n",
    "        self.best_estimator_ = None\n",
    "        \n",
    "    def _create_model(self, params):\n",
    "        \"\"\"Create model with consistent device configuration\"\"\"\n",
    "        model_params = {**self.base_params, **params}\n",
    "        return XGBRegressor(**model_params)\n",
    "        \n",
    "    def fit(self, X, y, groups=None):\n",
    "        keys = list(self.param_grid.keys())\n",
    "        values = list(self.param_grid.values())\n",
    "        param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing {len(param_combinations)} parameter combinations...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        best_score = float('-inf')\n",
    "        best_params = None\n",
    "        \n",
    "        for i, params in enumerate(param_combinations):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n[{i+1}/{len(param_combinations)}] Testing: {params}\")\n",
    "            \n",
    "            # Create model with current parameters\n",
    "            cv_model = self._create_model(params)\n",
    "            \n",
    "            # Perform cross-validation\n",
    "            cv_scores = []\n",
    "            for fold, (train_idx, val_idx) in enumerate(self.cv.split(X, y, groups=groups)):\n",
    "                try:\n",
    "                    if gpu_available:\n",
    "                        # Use GPU arrays for training and validation\n",
    "                        X_cv_train = X[train_idx]\n",
    "                        y_cv_train = y[train_idx]\n",
    "                        X_cv_val = X[val_idx]\n",
    "                        y_cv_val = y[val_idx]\n",
    "                        \n",
    "                        # Train on GPU\n",
    "                        cv_model.fit(X_cv_train, y_cv_train)\n",
    "                        pred = cv_model.predict(X_cv_val)\n",
    "                        \n",
    "                        # Convert to CPU for evaluation\n",
    "                        y_cv_val_cpu = cp.asnumpy(y_cv_val)\n",
    "                        pred_cpu = cp.asnumpy(pred) if hasattr(pred, 'device') else pred\n",
    "                        \n",
    "                    else:\n",
    "                        # Use CPU arrays but GPU XGBoost\n",
    "                        X_cv_train = X[train_idx]\n",
    "                        y_cv_train = y[train_idx]\n",
    "                        X_cv_val = X[val_idx]\n",
    "                        y_cv_val = y[val_idx]\n",
    "                        \n",
    "                        # Train on GPU (XGBoost handles CPU->GPU transfer internally)\n",
    "                        cv_model.fit(X_cv_train, y_cv_train)\n",
    "                        pred_cpu = cv_model.predict(X_cv_val)\n",
    "                        y_cv_val_cpu = y_cv_val\n",
    "                    \n",
    "                    # Calculate score\n",
    "                    score = -mean_absolute_error(y_cv_val_cpu, pred_cpu)\n",
    "                    cv_scores.append(score)\n",
    "                    \n",
    "                    if self.verbose:\n",
    "                        print(f\"  Fold {fold+1}: MAE = {-score:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error in fold {fold}: {e}\")\n",
    "                    cv_scores.append(float('-inf'))\n",
    "            \n",
    "            avg_score = np.mean(cv_scores)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"  Average CV score: {avg_score:.4f}\")\n",
    "\n",
    "            \n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = params\n",
    "            #Uncomment if you use verbose = 1\n",
    "            #     print(f\"  New best score!\")\n",
    "                \n",
    "        self.best_score_ = best_score\n",
    "        self.best_params_ = best_params\n",
    "        \n",
    "        # Train final model with best parameters\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Training final model with best parameters...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        self.best_estimator_ = self._create_model(best_params)\n",
    "        self.best_estimator_.fit(X, y)\n",
    "        \n",
    "        print(f\"Best parameters: {self.best_params_}\")\n",
    "        print(f\"Best CV score: {self.best_score_:.4f}\")\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b153bef-2367-4e39-9309-1721b7322ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xgboost_visualizations(model, predictions, y_test, test_data, feature_columns):\n",
    "    \"\"\"\n",
    "    Create and save XGBoost model visualizations\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    outPath = os.path.join(cwd, \"Plots\")\n",
    "    os.makedirs(outPath, exist_ok=True)\n",
    "    \n",
    "    # Scatter plot - Actual vs Predicted\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(y_test, predictions, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=3, label='Perfect Prediction')\n",
    "    plt.xlabel('Actual Earthquake Significance', fontsize=12)\n",
    "    plt.ylabel('Predicted Earthquake Significance', fontsize=12)\n",
    "    plt.title(f'XGBoost: Actual vs Predicted (R²={r2_score(y_test, predictions):.3f})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outPath, 'xgb_actual_vs_predicted_scatter.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {os.path.join(outPath, 'xgb_actual_vs_predicted_scatter.png')}\")\n",
    "    \n",
    "    # Time series plot of actual vs predicted\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    sample_size = min(200, len(y_test))\n",
    "    indices = np.arange(sample_size)\n",
    "    \n",
    "    plt.plot(indices, y_test[:sample_size], 'b-', label='Actual Significance', \n",
    "             linewidth=2, marker='o', markersize=3, alpha=0.7)\n",
    "    plt.plot(indices, predictions[:sample_size], 'r--', label='Predicted Significance', \n",
    "             linewidth=2, marker='s', markersize=3, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Sample Index', fontsize=12)\n",
    "    plt.ylabel('Earthquake Significance', fontsize=12)\n",
    "    plt.title('XGBoost: Actual vs Predicted Earthquake Significance Over Time', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(outPath, 'xgb_actual_vs_predicted_timeseries.png'), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {os.path.join(outPath, 'xgb_actual_vs_predicted_timeseries.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12731961-8cbf-4c93-9f0b-eacff2724485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(data, feature_columns, target_column, segment_column=None, \n",
    "                        param_grid=None, base_params=None, cv_splits=3):\n",
    "    # Load and prepare data\n",
    "    X_train, y_train, X_test, y_test, segments_train, segments_test, test_data = load_data_xgboost(\n",
    "        data, feature_columns, target_column, segment_column\n",
    "    )\n",
    "    \n",
    "    # Default base parameters\n",
    "    if base_params is None:\n",
    "        base_params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'tree_method': 'hist', #Try hist and exact\n",
    "            'device': 'cuda' if gpu_available else 'cpu',\n",
    "            'random_state': 42\n",
    "        }\n",
    "    \n",
    "    # Parameter search space\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'max_depth': [4, 6, 8],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0]\n",
    "        }\n",
    "    \n",
    "    # Create cross-validator\n",
    "    if segment_column:\n",
    "        group_cv = GroupKFold(n_splits=cv_splits)\n",
    "    else:\n",
    "        group_cv = KFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform hyperparameter tuning\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING HYPERPARAMETER TUNING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    xgb_cv = GPUGridSearchCV(\n",
    "        base_params=base_params,\n",
    "        param_grid=param_grid,\n",
    "        cv=group_cv,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        xgb_cv.fit(X_train, y_train, groups=segments_train)\n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining failed: {e}\")\n",
    "    \n",
    "    # Get the best model and make predictions\n",
    "    best_model = xgb_cv.best_estimator_\n",
    "    \n",
    "    print(\"\\nMaking predictions...\")\n",
    "    if gpu_available:\n",
    "        X_test_gpu = cp.array(X_test) if not hasattr(X_test, 'device') else X_test\n",
    "        predictions = best_model.predict(X_test_gpu)\n",
    "        predictions = cp.asnumpy(predictions) if hasattr(predictions, 'device') else predictions\n",
    "    else:\n",
    "        predictions = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"XGBOOST MODEL EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f'MAE: {mean_absolute_error(y_test, predictions):.4f}')\n",
    "    print(f'MSE: {mean_squared_error(y_test, predictions):.4f}')\n",
    "    print(f'RMSE: {np.sqrt(mean_squared_error(y_test, predictions)):.4f}')\n",
    "    print(f'R² Score: {r2_score(y_test, predictions):.4f}')\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_xgboost_visualizations(best_model, predictions, y_test, test_data, feature_columns)\n",
    "    \n",
    "    return best_model, predictions, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a703c1e3-99fc-4865-9fcc-e26f2298102f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EARTHQUAKE SIGNIFICANCE PREDICTION WITH XGBOOST\n",
      "============================================================\n",
      "\n",
      "Dataset shape: (1000, 19)\n",
      "Features: ['magnitude', 'cdi', 'mmi', 'tsunami', 'dmin', 'gap', 'depth', 'latitude', 'longitude']\n",
      "Target: sig (Earthquake Significance)\n",
      "\n",
      "============================================================\n",
      "XGBOOST DATA PREPARATION\n",
      "============================================================\n",
      "Features: ['magnitude', 'cdi', 'mmi', 'tsunami', 'dmin', 'gap', 'depth', 'latitude', 'longitude']\n",
      "Target: sig\n",
      "Train shape: (800, 9)\n",
      "Test shape: (200, 9)\n",
      "\n",
      "============================================================\n",
      "STARTING HYPERPARAMETER TUNING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Testing 108 parameter combinations...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training final model with best parameters...\n",
      "============================================================\n",
      "Best parameters: {'max_depth': 8, 'learning_rate': 0.05, 'n_estimators': 100, 'subsample': 0.8, 'colsample_bytree': 1.0}\n",
      "Best CV score: -104.6660\n",
      "\n",
      "Training completed successfully!\n",
      "\n",
      "Making predictions...\n",
      "\n",
      "============================================================\n",
      "XGBOOST MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "MAE: 25.3366\n",
      "MSE: 2782.9795\n",
      "RMSE: 52.7540\n",
      "R² Score: 0.8104\n",
      "Saved: /home/david/CS6900/EarthquakePredictionModel/Plots/xgb_actual_vs_predicted_scatter.png\n",
      "Saved: /home/david/CS6900/EarthquakePredictionModel/Plots/xgb_actual_vs_predicted_timeseries.png\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE!\n",
      "Check the 'Plots' folder for visualizations:\n",
      "   - Actual vs Predicted scatter plot\n",
      "   - Time series comparison\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EARTHQUAKE SIGNIFICANCE PREDICTION WITH XGBOOST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load earthquake data\n",
    "    earthquake_data = pd.read_csv('./datasets/earthquake_1995-2023.csv')\n",
    "    feature_columns = ['magnitude', 'cdi', 'mmi', 'tsunami', 'dmin', 'gap', 'depth', 'latitude', 'longitude']\n",
    "    target_column = 'sig'\n",
    "    \n",
    "    print(f\"\\nDataset shape: {earthquake_data.shape}\")\n",
    "    print(f\"Features: {feature_columns}\")\n",
    "    print(f\"Target: {target_column} (Earthquake Significance)\")\n",
    "    \n",
    "    # Prepare parameter grid\n",
    "    xgb_param_grid = {\n",
    "        'max_depth': [4, 6, 8],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    xgb_model, xgb_predictions, xgb_y_test = train_xgboost_model(\n",
    "        data=earthquake_data,\n",
    "        feature_columns=feature_columns,\n",
    "        target_column=target_column,\n",
    "        segment_column=None,\n",
    "        param_grid=xgb_param_grid,\n",
    "        cv_splits=3\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"Check the 'Plots' folder for visualizations:\")\n",
    "    print(\"   - Actual vs Predicted scatter plot\")\n",
    "    print(\"   - Time series comparison\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e813f3-aa6d-46b3-aa1c-0dd85e0f7c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
